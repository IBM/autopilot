# Default values for the Autopilot DaemonSet.
# This is a YAML-formatted file.
# Declare variables to be passed into your templates.
image:
  repository: quay.io/autopilot/autopilot
  pullPolicy: Always

# Workers for concurrent tasks. Defaults to 0 which uses 2*number_of_logical_CPU_cores (depends on the resource limits).
workers: 2

# Bandwidth threshold below which PCIe links are considered defective (Gb/s)
# It is recommended to set a threshold that is 25% or lower of the expected peak PCIe bandwidth capability, which maps to maximum peak from 16 lanes to 4 lanes. For example, for a PCIe Gen4x16, reported peak bandwidth is 63GB/s. A degradation at 25% is 15.75GB/s, which corresponds to PCIe Gen4x4. The measured bandwidth is expected to be at least 80% of the expected peak PCIe generation bandwidth.
PCIeBW: 4

# Timer for periodic checks, in interval format (e.g., 1h, 30m, 15s).
repeat: 1h

# Timer for periodic invasive checks (e.g., dcgmi diag -r 3), in interval format (e.g., 1h, 30m, 15s). Set to 0 to disable (for non nvidia gpu systems)
invasive: 4h

# Image pull secret if the image is in a private repository
pullSecrets:
  create: false
  name: autopilot-pull-secret
  imagePullSecretData: 
  
env:
# List of periodic checks to be executed every `repeat` hours.
# If not running on GPU nodes, pciebw,remapped,dcgm and gpupower can be removed
  - name: "PERIODIC_CHECKS"
    value: "pciebw,remapped,dcgm,ping,gpupower"
# Storage class name to test
  - name: "PVC_TEST_STORAGE_CLASS"
    value: ""
# List of GPU errors considered fatal, as a result of a dcgmi run. This is used to label nodes with extra WARN/EVICT labels. The list defaults to [PCIe,NVLink,ECC,GPU Memory] and refers to https://docs.nvidia.com/datacenter/dcgm/latest/user-guide/feature-overview.html#id3
  - name: "DCGM_FATAL_ERRORS"
    value: ""
# Invasive jobs (e.g., dcgm level 3), are executed as separate job. The job deletes itself by default after 30s. This parameter can be customized by the env variable below
  - name: "INVASIVE_JOB_TTLSEC"
    value: ""

service:
  port: 3333

annotations:
  # k8s.v1.cni.cncf.io/networks: multi-nic-network

nodeSelector:
  # nvidia.com/gpu.present: 'true'
  # nvidia.com/mig.config: 'all-disabled'

affinity:

# Running on GPU nodes only, will:
# 1) add the `nvidia.com/gpu.present: 'true'` label and 
# 2) enable the init container, which checks on the nvidia device plug-in to be setup
onlyOnGPUNodes: true

resources: 
  # We advice to not set cpu and memory limits. DCGM requires several GB of memory to run and it may OOMKill the pod
  limits:
    nvidia.com/gpu: 0
  requests:
    nvidia.com/gpu: 0

# klog configuration
loglevel: 2
# logfile: "/home/autopilot/data/report.log"

# optional remote storage. A PVC and secret must exist
additionalVolumeClaimTemplates:
  # - name: logdir
  #   persistentVolumeClaim:
  #     claimName: my-pvc
  # - name: autopilot-tls-secret
  #   secret:
  #     secretName: autopilot-webhook
additionalVolumeMounts:
  # - name: autopilot-tls-secret
  #   mountPath: "/etc/autopilot-tls-secret/tls"
  #   readOnly: true
  # - mountPath: /data
  #   name: logdir
